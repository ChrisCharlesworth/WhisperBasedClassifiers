{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","authorship_tag":"ABX9TyNS/lZIUcbAWZ/VwH0trK2m"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import json\n","from torch.utils.data import Dataset, DataLoader, Subset\n","from sklearn.model_selection import train_test_split, KFold\n","from sklearn.metrics import f1_score, jaccard_score, matthews_corrcoef, hamming_loss\n","from sklearn.preprocessing import LabelBinarizer\n","import os\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","import time\n","from datetime import datetime\n","import torch.nn.functional as F\n","from google.colab import drive\n","from sklearn.metrics import confusion_matrix\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"25ZHGXaWFhCq","executionInfo":{"status":"ok","timestamp":1718657764835,"user_tz":-120,"elapsed":2142,"user":{"displayName":"Christopher Charlesworth","userId":"15045930996748643945"}},"outputId":"1268a2bb-397a-4483-cfc3-c60a584a6df7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IuHN1sFUFYwq"},"outputs":[],"source":["\n","output_file_path = \"/content/drive/My Drive/CSE/Year 3/ResearchProject/Dataset/Normal_Runs/cnn_output.txt\"\n","# Log the start time\n","with open(output_file_path, 'w') as file:\n","    start_time = datetime.now()\n","    file.write(f\"Start time: {start_time}\\n\")\n","    file.flush()\n","\n","    # # Load your data\n","    # current_dir = os.getcwd()\n","    # parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n","    # embeddings_path = os.path.join(parent_dir, \"/content/drive/My Drive/CSE/Year 3/ResearchProject/Dataset/TORGO_Embeddings_Normal_Cut_New.json\")\n","\n","    # # Local Dataset\n","    # # current_dir = os.getcwd()\n","    # # parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n","    # # embeddings_path = os.path.join(parent_dir, \"Embeddings_5.5Seconds.json\")\n","\n","    # file.write('Opening file to read\\n')\n","    # with open(embeddings_path, 'r') as f:\n","    #     data = json.load(f)\n","\n","    # file_names = [entry['fileName'] for entry in data]\n","    # # numpy_arrays = [np.array(entry['numpyArray']) for entry in data]\n","    # # tensor_arrays = [torch.tensor(arr).float() for arr in numpy_arrays]\n","    # tensor_arrays = [torch.tensor(np.array(entry['numpyArray'])).float() for entry in data]\n","    # file.write('File has been read\\n')\n","    # file.flush()\n","    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    # class_names = []\n","    # tensor_arrays_balanced = []\n","\n","    # for i, file_name in enumerate(file_names):\n","    #   if ('MC01' in file_name or 'FCO1' in file_name or 'MC04/Session1' in file_name):\n","    #       class_names.append('Healthy')\n","    #       tensor_arrays_balanced.append(tensor_arrays[i])\n","    #   elif ('F04' in file_name or 'M03' in file_name):\n","    #       class_names.append('VeryLow')\n","    #       tensor_arrays_balanced.append(tensor_arrays[i])\n","    #   elif ('M05' in file_name or 'F03' in file_name ):\n","    #       class_names.append('Low')\n","    #       tensor_arrays_balanced.append(tensor_arrays[i])\n","    #   elif ('F01' in file_name or 'M01' in file_name\n","    #         or 'M02' in file_name or 'M04' in file_name):\n","    #       class_names.append('Medium')\n","    #       tensor_arrays_balanced.append(tensor_arrays[i])\n","\n","    # tensor_arrays = tensor_arrays_balanced\n","    # # tensor_arrays.to(device)\n","    # unique_class_names = ['Healthy', 'Low', 'VeryLow', 'Medium']\n","    # class_to_number = {class_name: i for i, class_name in enumerate(unique_class_names)}\n","    # class_numbers = [class_to_number[name] for name in class_names]\n","\n","    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    if torch.cuda.is_available():\n","        file.write(\"RUNNING ON GPU\\n\")\n","        file.flush()\n","\n","    else:\n","        file.write(\"Running ON CPU\\n\")\n","        file.flush()\n","\n","    class CustomEmbeddingDataset(Dataset):\n","        def __init__(self, embeddings, class_numbers):\n","            self.embeddings = embeddings\n","            self.class_numbers = class_numbers\n","\n","        def __len__(self):\n","            return len(self.embeddings)\n","\n","        def __getitem__(self, index):\n","            batch_embeddings = self.embeddings[index]\n","            batch_class_numbers = self.class_numbers[index]\n","            tensor_batch_class_numbers = torch.tensor(batch_class_numbers)\n","            return batch_embeddings, tensor_batch_class_numbers\n","\n","    class ConvNet(nn.Module):\n","        def __init__(self):\n","            super(ConvNet, self).__init__()\n","            # 1 since only black and white input of image\n","            # 6 is the number of outputs\n","            # 5 is the kernel size\n","            self.conv1 = nn.Conv2d(1, 16, kernel_size = 3)\n","            # width = (375 - 3) / 1 + 1= 373\n","            # height = (1280 -3) / 1  + 1= 1278\n","            self.pool = nn.MaxPool2d(kernel_size=2, stride = 2)\n","            # width = roundDown((373 - 2) / 2) + 1 = 186\n","            # height = roundDown((1278 - 2 ) / 2) + 1 = 639\n","            self.conv2 = nn.Conv2d(16, 32, kernel_size = 3)\n","            # width = (186 - 3) / 1 + 1 = 184\n","            # height = (639 - 3) / 1  + 1 = 637\n","\n","            # apply pool again\n","            # width = roundDown((184 - 2) / 2)  + 1 = 92\n","            # height = roundDown(637 - 2) / 2 + 1) = 318\n","\n","            self.conv3 = nn.Conv2d(32, 64, kernel_size = 5)\n","            # width = (92 - 5) / 1 + 1 = 88\n","            # height = (318 - 5) / 1 + 1 = 314\n","\n","            # apply pool again\n","            # width = roundDown((88 - 2) / 2)  + 1  = 44\n","            # height = (314 - 2) / 2 + 1 = 157\n","\n","            self.conv4 = nn.Conv2d(64, 128, kernel_size = 5)\n","            # width = roundDown(44 - 5) / 1)  + 1  = 40\n","            # height = (157 -5) / 1 + 1 = 153\n","\n","            # apply pool again\n","            # width = roundDown((40 -2) / 2) + 1 = 20\n","            # height = roundDown((153 - 2) / 2 ) + 1 = 76\n","\n","            self.fc1 = nn.Linear(20 * 76 * 128, 4)\n","\n","        def forward(self, x):\n","            # -> n, 3, 32, 32\n","            x = self.pool(F.relu(self.conv1(x)))\n","            x = self.pool(F.relu(self.conv2(x)))\n","            x = self.pool(F.relu(self.conv3(x)))\n","            x = self.pool(F.relu(self.conv4(x)))\n","\n","            x = x.view(-1, 20 * 76 * 128)\n","            # x = F.softmax(self.fc1(x))\n","            x = self.fc1(x)\n","            return x\n","\n","    input_size = 1280\n","    hidden_size = 128\n","    num_layers = 2\n","    num_classes = 4\n","    num_epochs = 100\n","    batch_size = 4\n","    initial_learning_rate = 0.01\n","    patience = 18\n","    k_folds = 10\n","    # patience = 2\n","    # k_folds = 2\n","    sequence_length = 375\n","    aggregate_cm = np.zeros((num_classes, num_classes), dtype=int)\n","\n","    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n","\n","    dataset = CustomEmbeddingDataset(tensor_arrays, class_numbers)\n","\n","    fold_results = []\n","    file.write(\"TRAINING THE MODEL\\n\")\n","    file.flush()\n","\n","    for fold, (train_idx, test_idx) in enumerate(kf.split(dataset)):\n","        output_string = \"FOLD NUMBER \" + str(fold + 1) + \"\\n\"\n","        file.write(output_string)\n","        file.flush()\n","        train_subset = Subset(dataset, train_idx)\n","        test_subset = Subset(dataset, test_idx)\n","\n","        train_validation_x = [x[0] for x in train_subset]\n","        train_validation_y = [x[1].item() for x in train_subset]\n","        x_test = [x[0] for x in test_subset]\n","        y_test = [x[1].item() for x in test_subset]\n","        x_train, x_validation, y_train, y_validation = train_test_split(train_validation_x, train_validation_y, test_size=0.1, random_state=42)\n","\n","        train_data_set = CustomEmbeddingDataset(x_train, y_train)\n","        validation_data_set = CustomEmbeddingDataset(x_validation, y_validation)\n","        test_data_set = CustomEmbeddingDataset(x_test, y_test)\n","\n","        train_loader = DataLoader(train_data_set, batch_size=batch_size, shuffle=True)\n","        validation_loader = DataLoader(validation_data_set, batch_size=batch_size, shuffle=False)\n","        test_loader = DataLoader(test_data_set, batch_size=batch_size, shuffle=False)\n","\n","        model = ConvNet().to(device)\n","        criterion = nn.CrossEntropyLoss()\n","        optimizer = torch.optim.SGD(model.parameters(), lr=initial_learning_rate)\n","\n","        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=6, min_lr=0.00001)\n","\n","        best_val_loss = float('inf')\n","        best_epoch = 0\n","\n","        for epoch in range(num_epochs):\n","            model.train()\n","            total_loss = 0\n","            for images, labels in train_loader:\n","                images = images.unsqueeze(1).to(device)\n","                labels = labels.to(device)\n","                outputs = model(images)\n","                loss = criterion(outputs, labels)\n","\n","                # Backward and optimize\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","                total_loss = total_loss + loss.item()\n","\n","            model.eval()\n","            val_loss = 0\n","            y_true_val, y_pred_val = [], []\n","            n_correct_val = 0\n","            n_samples_val = 0\n","            n_class_correct_val = [0 for _ in range(num_classes)]\n","            n_class_samples_val = [0 for _ in range(num_classes)]\n","            with torch.no_grad():\n","                for images, labels in validation_loader:\n","                    images = images.unsqueeze(1).to(device)\n","                    labels = labels.to(device)\n","                    outputs = model(images)\n","                    loss = criterion(outputs, labels)\n","                    val_loss = val_loss + loss.item()\n","                    _, predicted = torch.max(outputs, 1)\n","                    y_true_val.extend(labels.cpu().numpy())\n","                    y_pred_val.extend(predicted.cpu().numpy())\n","\n","                    n_samples_val += labels.size(0)\n","                    n_correct_val += (predicted == labels).sum().item()\n","                    for i in range(len(labels)):\n","                        label = labels[i].item()\n","                        pred = predicted[i].item()\n","                        if label == pred:\n","                            n_class_correct_val[label] += 1\n","                        n_class_samples_val[label] += 1\n","\n","            model.eval()\n","            y_true, y_pred = [], []\n","            n_correct = 0\n","            n_samples = 0\n","            n_class_correct = [0 for _ in range(num_classes)]\n","            n_class_samples = [0 for _ in range(num_classes)]\n","            with torch.no_grad():\n","                for images, labels in test_loader:\n","                    images = images.unsqueeze(1)\n","                    images = images.to(device)\n","                    labels = labels.to(device)\n","                    outputs = model(images)\n","                    # max returns (value ,index)\n","                    _, predicted = torch.max(outputs, 1)\n","                    y_true.extend(labels.cpu().numpy())\n","                    y_pred.extend(predicted.cpu().numpy())\n","\n","                    n_samples += labels.size(0)\n","                    n_correct += (predicted == labels).sum().item()\n","                    for i in range(len(labels)):\n","                        label = labels[i].item()\n","                        pred = predicted[i].item()\n","                        if label == pred:\n","                            n_class_correct[label] += 1\n","                        n_class_samples[label] += 1\n","\n","            acc = 100.0 * n_correct / n_samples\n","            file.write(f\"Test Accuracy of the network: {acc} %\\n\")\n","            file.flush()\n","            acc_val = 100.0 * n_correct_val / n_samples_val\n","            file.write(f\"Validation Accuracy of the network: {acc_val} %\\n\")\n","\n","            val_loss /= len(validation_loader)\n","            file.write(f\"Epoch {epoch+1}, Validation Loss: {val_loss:.4f}, Learning Rate: {optimizer.param_groups[0]['lr']}\\n\")\n","            current_time = datetime.now()\n","            file.write(f\"Start time: {current_time}\\n\")\n","            file.flush()\n","\n","            scheduler.step(val_loss)\n","\n","            if val_loss < best_val_loss:\n","                best_val_loss = val_loss\n","                best_epoch = epoch\n","                torch.save(model.state_dict(), f'CNN_best_model_fold_{fold}.pth')\n","            elif epoch - best_epoch >= patience:\n","                file.write(f\"Early stopping at epoch {epoch + 1}\\n\")\n","                file.flush()\n","                break\n","\n","        model.load_state_dict(torch.load(f\"CNN_best_model_fold_{fold}.pth\"))\n","\n","        # Evaluation\n","        model.eval()\n","        y_true, y_pred = [], []\n","        n_correct = 0\n","        n_samples = 0\n","        n_class_correct = [0 for _ in range(num_classes)]\n","        n_class_samples = [0 for _ in range(num_classes)]\n","        with torch.no_grad():\n","            for images, labels in test_loader:\n","                images = images.unsqueeze(1)\n","                images = images.to(device)\n","                labels = labels.to(device)\n","                outputs = model(images)\n","                # max returns (value ,index)\n","                _, predicted = torch.max(outputs, 1)\n","                y_true.extend(labels.cpu().numpy())\n","                y_pred.extend(predicted.cpu().numpy())\n","\n","                n_samples += labels.size(0)\n","                n_correct += (predicted == labels).sum().item()\n","                for i in range(len(labels)):\n","                    label = labels[i].item()\n","                    pred = predicted[i].item()\n","                    if label == pred:\n","                        n_class_correct[label] += 1\n","                    n_class_samples[label] += 1\n","\n","            acc = 100.0 * n_correct / n_samples\n","            file.write(f\"Accuracy of the network: {acc} %\\n\")\n","            file.flush()\n","\n","            class_accs = []\n","            for i in range(num_classes):\n","                if n_class_samples[i] != 0:\n","                    current_acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n","                    class_accs.append(current_acc)\n","                    file.write(f\"Accuracy of {unique_class_names[i]}: {current_acc} %\\n\")\n","                    file.flush()\n","                else:\n","                    class_accs.append(0.0)\n","                    file.write(f\"Accuracy of {unique_class_names[i]}: No samples\\n\")\n","                    file.flush()\n","\n","        cm = confusion_matrix(y_true, y_pred)\n","        f1_micro = f1_score(y_true, y_pred, average='micro')\n","        f1_macro = f1_score(y_true, y_pred, average='macro')\n","        f1_weighted = f1_score(y_true, y_pred, average='weighted')\n","        jaccard_micro = jaccard_score(y_true, y_pred, average='micro')\n","        jaccard_macro = jaccard_score(y_true, y_pred, average='macro')\n","        jaccard_weighted = jaccard_score(y_true, y_pred, average='weighted')\n","        lb = LabelBinarizer()\n","        y_true_bin = lb.fit_transform(y_true)\n","        y_pred_bin = lb.transform(y_pred)\n","        mccs = [matthews_corrcoef(y_true_bin[:, i], y_pred_bin[:, i]) for i in range(len(lb.classes_))]\n","        average_mcc = sum(mccs) / len(mccs)\n","        hl = hamming_loss(y_true_bin, y_pred_bin)\n","\n","        file.write(f\"Fold: {fold}\\n\")\n","        file.write(f\"Confusion Matrix:\\n{cm}\\n\")\n","        file.write(f\"Accuracy: {acc:.2f}%\\n\")\n","        file.write(f\"F1 Micro: {f1_micro:.4f}\\n\")\n","        file.write(f\"F1 Macro: {f1_macro:.4f}\\n\")\n","        file.write(f\"F1 Weighted: {f1_weighted:.4f}\\n\")\n","        file.write(f\"Jaccard Micro: {jaccard_micro:.4f}\\n\")\n","        file.write(f\"Jaccard Macro: {jaccard_macro:.4f}\\n\")\n","        file.write(f\"Jaccard Weighted: {jaccard_weighted:.4f}\\n\")\n","        file.write(f\"MCC: {average_mcc:.4f}\\n\")\n","        file.write(f\"Hamming Loss: {hl:.4f}\\n\")\n","        file.flush()\n","\n","        aggregate_cm += cm\n","        fold_results.append({\n","            'fold': fold,\n","            'accuracy': acc,\n","            'class_accuracy': class_accs,\n","            'f1_micro': f1_micro,\n","            'f1_macro': f1_macro,\n","            'f1_weighted': f1_weighted,\n","            'jaccard_micro': jaccard_micro,\n","            'jaccard_macro': jaccard_macro,\n","            'jaccard_weighted': jaccard_weighted,\n","            'mcc': average_mcc,\n","            'hamming_loss': hl\n","        })\n","\n","    # Compute and print the average metrics\n","    avg_accuracy = sum(result['accuracy'] for result in fold_results) / k_folds\n","    avg_class_accuracy = [sum(result['class_accuracy'][i] for result in fold_results) / k_folds for i in range(num_classes)]\n","    avg_f1_micro = sum(result['f1_micro'] for result in fold_results) / k_folds\n","    avg_f1_macro = sum(result['f1_macro'] for result in fold_results) / k_folds\n","    avg_f1_weighted = sum(result['f1_weighted'] for result in fold_results) / k_folds\n","    avg_jaccard_micro = sum(result['jaccard_micro'] for result in fold_results) / k_folds\n","    avg_jaccard_macro = sum(result['jaccard_macro'] for result in fold_results) / k_folds\n","    avg_jaccard_weighted = sum(result['jaccard_weighted'] for result in fold_results) / k_folds\n","    avg_mcc = sum(result['mcc'] for result in fold_results) / k_folds\n","    avg_hamming_loss = sum(result['hamming_loss'] for result in fold_results) / k_folds\n","\n","    file.write(f\"Average accuracy: {avg_accuracy:.2f}%\\n\")\n","    file.flush()\n","    for i in range(num_classes):\n","        file.write(f\"Average accuracy of {unique_class_names[i]}: {avg_class_accuracy[i]:.2f}%\\n\")\n","    file.write(f\"Average F1 Score (Micro): {avg_f1_micro:.4f}\\n\")\n","    file.write(f\"Average F1 Score (Macro): {avg_f1_macro:.4f}\\n\")\n","    file.write(f\"Average F1 Score (Weighted): {avg_f1_weighted:.4f}\\n\")\n","    file.write(f\"Average Jaccard Score (Micro): {avg_jaccard_micro:.4f}\\n\")\n","    file.write(f\"Average Jaccard Score (Macro): {avg_jaccard_macro:.4f}\\n\")\n","    file.write(f\"Average Jaccard Score (Weighted): {avg_jaccard_weighted:.4f}\\n\")\n","    file.write(f\"Average MCC: {avg_mcc:.4f}\\n\")\n","    file.write(f\"Average Hamming Loss: {avg_hamming_loss:.4f}\\n\")\n","    file.write(f\"Cumulative Confusion Matrix: {aggregate_cm}\\n\")\n","\n","    # Log the end time\n","    end_time = datetime.now()\n","    file.write(f\"End time: {end_time}\\n\")\n","\n","    # Optionally, you can also log the duration\n","    duration = end_time - start_time\n","    file.write(f\"Duration: {duration}\\n\")\n"]},{"cell_type":"code","source":[],"metadata":{"id":"Sq3YU8yfJjuA"},"execution_count":null,"outputs":[]}]}