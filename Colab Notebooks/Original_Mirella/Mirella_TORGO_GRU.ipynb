{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V28","mount_file_id":"1Ao6WhTnhPky4sutGTXrOZSRJfHhcFezE","authorship_tag":"ABX9TyNyiLt3ZsaTAT1l45tEhQhK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","source":["import json\n","from google.colab import drive\n","import librosa\n","from transformers import WhisperForConditionalGeneration, AutoProcessor\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p17U0zVjR7UQ","executionInfo":{"status":"ok","timestamp":1718277754108,"user_tz":-120,"elapsed":34356,"user":{"displayName":"Christopher Charlesworth","userId":"15045930996748643945"}},"outputId":"2508f16b-39db-4b0c-a7a8-bba0f05acdea"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c4I7fun9RnBL"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import json\n","from torch.utils.data import Dataset, DataLoader, Subset\n","from sklearn.model_selection import train_test_split, KFold\n","from sklearn.metrics import f1_score, jaccard_score, matthews_corrcoef, hamming_loss\n","from sklearn.preprocessing import LabelBinarizer\n","import os\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","import time\n","from datetime import datetime\n","\n","# Log the start time\n","output_file_path = \"/content/drive/My Drive/CSE/Year 3/ResearchProject/Dataset/Mirella_Runs/gru_output.txt\"\n","with open(output_file_path, 'w') as file:\n","    start_time = datetime.now()\n","    file.write(f\"Start time: {start_time}\\n\")\n","    file.flush()\n","\n","    # Load your data\n","    current_dir = os.getcwd()\n","    parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n","    embeddings_path = os.path.join(parent_dir, \"/content/drive/My Drive/CSE/Year 3/ResearchProject/Dataset/TORGO_Embeddings_Mirella.json\")\n","\n","    # Local Dataset\n","    # current_dir = os.getcwd()\n","    # parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n","    # embeddings_path = os.path.join(parent_dir, \"Embeddings_5.5Seconds.json\")\n","\n","    file.write('Opening file to read\\n')\n","    with open(embeddings_path, 'r') as f:\n","        data = json.load(f)\n","\n","    file_names = [entry['fileName'] for entry in data]\n","    numpy_arrays = [np.array(entry['numpyArray']) for entry in data]\n","    tensor_arrays = [torch.tensor(arr).float() for arr in numpy_arrays]\n","    file.write('File has been read\\n')\n","    file.flush()\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    class_names = []\n","    tensor_arrays_balanced = []\n","\n","    for i, file_name in enumerate(file_names):\n","        if ('MC01' in file_name or 'FCO1' in file_name or 'MC04/Session2' in file_name):\n","            class_names.append('Healthy')\n","            tensor_arrays_balanced.append(tensor_arrays[i])\n","        elif ('F04' in file_name or 'M03' in file_name):\n","            class_names.append('VeryLow')\n","            tensor_arrays_balanced.append(tensor_arrays[i])\n","        elif ('F01' in file_name or 'M05' in file_name or 'F03/Session2' in file_name or 'F03/Session3' in file_name ):\n","            class_names.append('Low')\n","            tensor_arrays_balanced.append(tensor_arrays[i])\n","        elif ('M01' in file_name or 'M01' in file_name or 'M02' in file_name or 'M04' in file_name):\n","            class_names.append('Medium')\n","            tensor_arrays_balanced.append(tensor_arrays[i])\n","\n","    tensor_arrays = tensor_arrays_balanced\n","\n","    unique_class_names = ['Healthy', 'Low', 'VeryLow', 'Medium']\n","    class_to_number = {class_name: i for i, class_name in enumerate(unique_class_names)}\n","    class_numbers = [class_to_number[name] for name in class_names]\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    if torch.cuda.is_available():\n","        file.write(\"RUNNING ON GPU\\n\")\n","        file.flush()\n","\n","    else:\n","        file.write(\"Running ON CPU\\n\")\n","        file.flush()\n","\n","    class CustomEmbeddingDataset(Dataset):\n","        def __init__(self, embeddings, class_numbers):\n","            self.embeddings = embeddings\n","            self.class_numbers = class_numbers\n","\n","        def __len__(self):\n","            return len(self.embeddings)\n","\n","        def __getitem__(self, index):\n","            batch_embeddings = self.embeddings[index]\n","            batch_class_numbers = self.class_numbers[index]\n","            tensor_batch_class_numbers = torch.tensor(batch_class_numbers)\n","            return batch_embeddings, tensor_batch_class_numbers\n","\n","    class RNN(nn.Module):\n","        def __init__(self, input_size, hidden_size, num_layers, num_classes):\n","            super(RNN, self).__init__()\n","            self.num_layers = num_layers\n","            self.hidden_size = hidden_size\n","            self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n","            self.fc = nn.Linear(hidden_size, num_classes)\n","\n","        def forward(self, x):\n","            h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","            out, _ = self.rnn(x, h0)\n","            out = out[:, -1, :]\n","            out = self.fc(out)\n","            return out\n","\n","    input_size = 1280\n","    hidden_size = 128\n","    num_layers = 2\n","    num_classes = 4\n","    num_epochs = 100\n","    batch_size = 4\n","    initial_learning_rate = 0.01\n","    patience = 13\n","    k_folds = 10\n","    # patience = 2\n","    # k_folds = 2\n","    sequence_length = 375\n","\n","    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n","\n","    dataset = CustomEmbeddingDataset(tensor_arrays, class_numbers)\n","\n","    fold_results = []\n","    file.write(\"TRAINING THE MODEL\\n\")\n","    file.flush()\n","\n","    for fold, (train_idx, test_idx) in enumerate(kf.split(dataset)):\n","        output_string = \"FOLD NUMBER \" + str(fold + 1) + \"\\n\"\n","        file.write(output_string)\n","        file.flush()\n","        train_subset = Subset(dataset, train_idx)\n","        test_subset = Subset(dataset, test_idx)\n","\n","        train_validation_x = [x[0] for x in train_subset]\n","        train_validation_y = [x[1].item() for x in train_subset]\n","        x_test = [x[0] for x in test_subset]\n","        y_test = [x[1].item() for x in test_subset]\n","        x_train, x_validation, y_train, y_validation = train_test_split(train_validation_x, train_validation_y, test_size=0.1, random_state=42)\n","\n","        train_data_set = CustomEmbeddingDataset(x_train, y_train)\n","        validation_data_set = CustomEmbeddingDataset(x_validation, y_validation)\n","        test_data_set = CustomEmbeddingDataset(x_test, y_test)\n","\n","        train_loader = DataLoader(train_data_set, batch_size=batch_size, shuffle=True)\n","        validation_loader = DataLoader(validation_data_set, batch_size=batch_size, shuffle=False)\n","        test_loader = DataLoader(test_data_set, batch_size=batch_size, shuffle=False)\n","\n","        model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n","        criterion = nn.CrossEntropyLoss()\n","        optimizer = optim.Adam(model.parameters(), lr=initial_learning_rate)\n","\n","        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=4, min_lr=0.00001)\n","\n","        best_val_loss = float('inf')\n","        best_epoch = 0\n","\n","        for epoch in range(num_epochs):\n","            model.train()\n","            total_loss = 0\n","            for images, labels in train_loader:\n","                images = images.reshape(-1, sequence_length, input_size).to(device)\n","                labels = labels.to(device)\n","\n","                outputs = model(images)\n","                loss = criterion(outputs, labels)\n","\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","                total_loss += loss.item()\n","\n","            model.eval()\n","            val_loss = 0\n","            with torch.no_grad():\n","                for images, labels in validation_loader:\n","                    images = images.reshape(-1, sequence_length, input_size).to(device)\n","                    labels = labels.to(device)\n","                    outputs = model(images)\n","                    loss = criterion(outputs, labels)\n","                    val_loss += loss.item()\n","\n","            val_loss /= len(validation_loader)\n","            file.write(f\"Epoch {epoch+1}, Validation Loss: {val_loss:.4f}, Learning Rate: {optimizer.param_groups[0]['lr']}\\n\")\n","            current_time = datetime.now()\n","            file.write(f\"Start time: {current_time}\\n\")\n","            file.flush()\n","\n","            scheduler.step(val_loss)\n","\n","            if val_loss < best_val_loss:\n","                best_val_loss = val_loss\n","                best_epoch = epoch\n","                torch.save(model.state_dict(), f'GRU_best_model_fold_{fold}.pth')\n","            elif epoch - best_epoch >= patience:\n","                file.write(f\"Early stopping at epoch {epoch + 1}\\n\")\n","                file.flush()\n","                break\n","\n","        model.load_state_dict(torch.load(f\"GRU_best_model_fold_{fold}.pth\"))\n","\n","        # Evaluation\n","        model.eval()\n","        y_true, y_pred = [], []\n","        n_correct = 0\n","        n_samples = 0\n","        n_class_correct = [0 for _ in range(num_classes)]\n","        n_class_samples = [0 for _ in range(num_classes)]\n","        with torch.no_grad():\n","            for images, labels in test_loader:\n","                images = images.reshape(-1, sequence_length, input_size).to(device)\n","                labels = labels.to(device)\n","                outputs = model(images)\n","                _, predicted = torch.max(outputs, 1)\n","                y_true.extend(labels.cpu().numpy())\n","                y_pred.extend(predicted.cpu().numpy())\n","\n","                n_samples += labels.size(0)\n","                n_correct += (predicted == labels).sum().item()\n","                for i in range(len(labels)):\n","                    label = labels[i].item()\n","                    pred = predicted[i].item()\n","                    if label == pred:\n","                        n_class_correct[label] += 1\n","                    n_class_samples[label] += 1\n","\n","            acc = 100.0 * n_correct / n_samples\n","            file.write(f\"Accuracy of the network: {acc} %\\n\")\n","            file.flush()\n","\n","            class_accs = []\n","            for i in range(num_classes):\n","                if n_class_samples[i] != 0:\n","                    current_acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n","                    class_accs.append(current_acc)\n","                    file.write(f\"Accuracy of {unique_class_names[i]}: {current_acc} %\\n\")\n","                    file.flush()\n","                else:\n","                    class_accs.append(0.0)\n","                    file.write(f\"Accuracy of {unique_class_names[i]}: No samples\\n\")\n","                    file.flush()\n","\n","        f1_micro = f1_score(y_true, y_pred, average='micro')\n","        f1_macro = f1_score(y_true, y_pred, average='macro')\n","        f1_weighted = f1_score(y_true, y_pred, average='weighted')\n","        jaccard_micro = jaccard_score(y_true, y_pred, average='micro')\n","        jaccard_macro = jaccard_score(y_true, y_pred, average='macro')\n","        jaccard_weighted = jaccard_score(y_true, y_pred, average='weighted')\n","        lb = LabelBinarizer()\n","        y_true_bin = lb.fit_transform(y_true)\n","        y_pred_bin = lb.transform(y_pred)\n","        mccs = [matthews_corrcoef(y_true_bin[:, i], y_pred_bin[:, i]) for i in range(len(lb.classes_))]\n","        average_mcc = sum(mccs) / len(mccs)\n","        hl = hamming_loss(y_true_bin, y_pred_bin)\n","\n","        file.write(f\"Fold: {fold}\\n\")\n","        file.write(f\"Accuracy: {acc:.2f}%\\n\")\n","        file.write(f\"F1 Micro: {f1_micro:.4f}\\n\")\n","        file.write(f\"F1 Macro: {f1_macro:.4f}\\n\")\n","        file.write(f\"F1 Weighted: {f1_weighted:.4f}\\n\")\n","        file.write(f\"Jaccard Micro: {jaccard_micro:.4f}\\n\")\n","        file.write(f\"Jaccard Macro: {jaccard_macro:.4f}\\n\")\n","        file.write(f\"Jaccard Weighted: {jaccard_weighted:.4f}\\n\")\n","        file.write(f\"MCC: {average_mcc:.4f}\\n\")\n","        file.write(f\"Hamming Loss: {hl:.4f}\\n\")\n","        file.flush()\n","\n","        fold_results.append({\n","            'fold': fold,\n","            'accuracy': acc,\n","            'class_accuracy': class_accs,\n","            'f1_micro': f1_micro,\n","            'f1_macro': f1_macro,\n","            'f1_weighted': f1_weighted,\n","            'jaccard_micro': jaccard_micro,\n","            'jaccard_macro': jaccard_macro,\n","            'jaccard_weighted': jaccard_weighted,\n","            'mcc': average_mcc,\n","            'hamming_loss': hl\n","        })\n","\n","    # Compute and print the average metrics\n","    avg_accuracy = sum(result['accuracy'] for result in fold_results) / k_folds\n","    avg_class_accuracy = [sum(result['class_accuracy'][i] for result in fold_results) / k_folds for i in range(num_classes)]\n","    avg_f1_micro = sum(result['f1_micro'] for result in fold_results) / k_folds\n","    avg_f1_macro = sum(result['f1_macro'] for result in fold_results) / k_folds\n","    avg_f1_weighted = sum(result['f1_weighted'] for result in fold_results) / k_folds\n","    avg_jaccard_micro = sum(result['jaccard_micro'] for result in fold_results) / k_folds\n","    avg_jaccard_macro = sum(result['jaccard_macro'] for result in fold_results) / k_folds\n","    avg_jaccard_weighted = sum(result['jaccard_weighted'] for result in fold_results) / k_folds\n","    avg_mcc = sum(result['mcc'] for result in fold_results) / k_folds\n","    avg_hamming_loss = sum(result['hamming_loss'] for result in fold_results) / k_folds\n","\n","    file.write(f\"Average accuracy: {avg_accuracy:.2f}%\\n\")\n","    file.flush()\n","    for i in range(num_classes):\n","        file.write(f\"Average accuracy of {unique_class_names[i]}: {avg_class_accuracy[i]:.2f}%\\n\")\n","    file.write(f\"Average F1 Score (Micro): {avg_f1_micro:.4f}\\n\")\n","    file.write(f\"Average F1 Score (Macro): {avg_f1_macro:.4f}\\n\")\n","    file.write(f\"Average F1 Score (Weighted): {avg_f1_weighted:.4f}\\n\")\n","    file.write(f\"Average Jaccard Score (Micro): {avg_jaccard_micro:.4f}\\n\")\n","    file.write(f\"Average Jaccard Score (Macro): {avg_jaccard_macro:.4f}\\n\")\n","    file.write(f\"Average Jaccard Score (Weighted): {avg_jaccard_weighted:.4f}\\n\")\n","    file.write(f\"Average MCC: {avg_mcc:.4f}\\n\")\n","    file.write(f\"Average Hamming Loss: {avg_hamming_loss:.4f}\\n\")\n","\n","    # Log the end time\n","    end_time = datetime.now()\n","    file.write(f\"End time: {end_time}\\n\")\n","\n","    # Optionally, you can also log the duration\n","    duration = end_time - start_time\n","    file.write(f\"Duration: {duration}\\n\")\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"3R12z8RCjSjw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"It1s5DqISM4I"},"execution_count":null,"outputs":[]}]}