{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import f1_score, jaccard_score, matthews_corrcoef, hamming_loss\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import os\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import time\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_dir = os.getcwd()\n",
    "# parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "# embeddings_path = os.path.join(parent_dir, \"Embeddings2-5.json\")\n",
    "\n",
    "# Local Dataset\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "embeddings_path = os.path.join(parent_dir, \"Embeddings_5.5Seconds.json\")\n",
    "\n",
    "# file.write('Opening file to read\\n')\n",
    "with open(embeddings_path, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 186\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[39m# Backward and optimize\u001b[39;00m\n\u001b[1;32m    185\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 186\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    187\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    188\u001b[0m total_loss \u001b[39m=\u001b[39m total_loss \u001b[39m+\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Desktop/Uni_Repos/Whisper/env/lib/python3.8/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/Uni_Repos/Whisper/env/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Log the start time\n",
    "with open('cnn_output.txt', 'w') as file:\n",
    "    start_time = datetime.now()\n",
    "    file.write(f\"Start time: {start_time}\\n\")\n",
    "    file.flush()\n",
    "\n",
    "    # # Load your data\n",
    "    # current_dir = os.getcwd()\n",
    "    # parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "    # embeddings_path = os.path.join(parent_dir, \"2-5Seconds/Embeddings2-5.json\")\n",
    "\n",
    "    # # Local Dataset\n",
    "    # # current_dir = os.getcwd()\n",
    "    # # parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "    # # embeddings_path = os.path.join(parent_dir, \"Embeddings_5.5Seconds.json\")\n",
    "\n",
    "    # file.write('Opening file to read\\n')\n",
    "    # with open(embeddings_path, 'r') as f:\n",
    "    #     data = json.load(f)\n",
    "\n",
    "    file_names = [entry['fileName'] for entry in data]\n",
    "    numpy_arrays = [np.array(entry['numpyArray'])[:125, :] for entry in data]\n",
    "    tensor_arrays = [torch.tensor(arr).float() for arr in numpy_arrays]\n",
    "    file.write('File has been read\\n')\n",
    "    file.flush()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    class_names = []\n",
    "    tensor_arrays_balanced = []\n",
    "\n",
    "    for i, file_name in enumerate(file_names): \n",
    "        if ('MC01' in file_name or 'FCO1' in file_name or 'MC04/Session2' in file_name):\n",
    "            class_names.append('Healthy')\n",
    "            tensor_arrays_balanced.append(tensor_arrays[i])\n",
    "        elif ('F04' in file_name or 'M03' in file_name):\n",
    "            class_names.append('VeryLow')\n",
    "            tensor_arrays_balanced.append(tensor_arrays[i])\n",
    "        elif ('F01' in file_name or 'M05' in file_name or 'F03/Session2' in file_name or 'F03/Session3' in file_name ):\n",
    "            class_names.append('Low')\n",
    "            tensor_arrays_balanced.append(tensor_arrays[i])\n",
    "        elif ('M01' in file_name or 'M01' in file_name or 'M02' in file_name or 'M04' in file_name):\n",
    "            class_names.append('Medium')\n",
    "            tensor_arrays_balanced.append(tensor_arrays[i])\n",
    "\n",
    "    tensor_arrays = tensor_arrays_balanced\n",
    "\n",
    "    unique_class_names = ['Healthy', 'Low', 'VeryLow', 'Medium']\n",
    "    class_to_number = {class_name: i for i, class_name in enumerate(unique_class_names)}\n",
    "    class_numbers = [class_to_number[name] for name in class_names]\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        file.write(\"RUNNING ON GPU\\n\")\n",
    "        file.flush()\n",
    "\n",
    "    else:\n",
    "        file.write(\"Running ON CPU\\n\")\n",
    "        file.flush()\n",
    "\n",
    "    class CustomEmbeddingDataset(Dataset):\n",
    "        def __init__(self, embeddings, class_numbers):\n",
    "            self.embeddings = embeddings\n",
    "            self.class_numbers = class_numbers\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.embeddings)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            batch_embeddings = self.embeddings[index]\n",
    "            batch_class_numbers = self.class_numbers[index]\n",
    "            tensor_batch_class_numbers = torch.tensor(batch_class_numbers)\n",
    "            return batch_embeddings, tensor_batch_class_numbers\n",
    "\n",
    "    class ConvNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(ConvNet, self).__init__()\n",
    "            # 1 since only black and white input of image\n",
    "            # 6 is the number of outputs\n",
    "            # 5 is the kernel size \n",
    "            self.conv1 = nn.Conv2d(1, 16, kernel_size = 3)\n",
    "            # width = (125 - 3) / 1 + 1= 123\n",
    "            # height = (1280 -3) / 1  + 1= 1278\n",
    "            self.pool = nn.MaxPool2d(kernel_size=2, stride = 2)\n",
    "            # width = roundDown((123 - 2) / 2) + 1 = 61\n",
    "            # height = roundDown((1278 - 2 ) / 2) + 1 = 639\n",
    "            self.conv2 = nn.Conv2d(16, 32, kernel_size = 3)\n",
    "            # width = (61 - 3) / 1 + 1 = 59\n",
    "            # height = (639 - 3) / 1  + 1 = 637\n",
    "\n",
    "            # apply pool again \n",
    "            # width = roundDown((59 - 2) / 2)  + 1 = 29\n",
    "            # height = roundDown(637 - 2) / 2 + 1) = 318\n",
    "\n",
    "            self.conv3 = nn.Conv2d(32, 64, kernel_size = 5)\n",
    "            # width = (29 - 5) / 1 + 1 = 25\n",
    "            # height = (318 - 5) / 1 + 1 = 314\n",
    "\n",
    "            # apply pool again\n",
    "            # width = roundDown((25 - 2) / 2)  + 1  = 12\n",
    "            # height = (314 - 2) / 2 + 1 = 157\n",
    "\n",
    "            self.conv4 = nn.Conv2d(64, 128, kernel_size = 5)\n",
    "            # width = roundDown(12 - 5) / 1)  + 1  = 8\n",
    "            # height = (157 -5) / 1 + 1 = 153\n",
    "\n",
    "            # apply pool again\n",
    "            # width = roundDown((8 -2) / 2) + 1 = 4 \n",
    "            # height = roundDown((153 - 2) / 2 ) + 1 = 76\n",
    "\n",
    "            self.fc1 = nn.Linear(4 * 76 * 128, 4)\n",
    "        def forward(self, x):\n",
    "            # -> n, 3, 32, 32\n",
    "            x = self.pool(F.relu(self.conv1(x)))  \n",
    "            x = self.pool(F.relu(self.conv2(x)))  \n",
    "            x = self.pool(F.relu(self.conv3(x)))  \n",
    "            x = self.pool(F.relu(self.conv4(x)))  \n",
    "\n",
    "            x = x.view(-1, 4 * 76 * 128)           \n",
    "            # x = F.softmax(self.fc1(x))       \n",
    "            x = self.fc1(x)                          \n",
    "            return x\n",
    "\n",
    "    input_size = 1280\n",
    "    hidden_size = 128\n",
    "    num_layers = 2\n",
    "    num_classes = 4\n",
    "    num_epochs = 100\n",
    "    batch_size = 4\n",
    "    initial_learning_rate = 0.01\n",
    "    patience = 13\n",
    "    k_folds = 10\n",
    "    # patience = 2\n",
    "    # k_folds = 2\n",
    "    sequence_length = 375\n",
    "\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    dataset = CustomEmbeddingDataset(tensor_arrays, class_numbers)\n",
    "\n",
    "    fold_results = []\n",
    "    file.write(\"TRAINING THE MODEL\\n\")\n",
    "    file.flush()\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(dataset)):\n",
    "        output_string = \"FOLD NUMBER \" + str(fold + 1) + \"\\n\"\n",
    "        file.write(output_string)\n",
    "        file.flush()\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        test_subset = Subset(dataset, test_idx)\n",
    "        \n",
    "        train_validation_x = [x[0] for x in train_subset]\n",
    "        train_validation_y = [x[1].item() for x in train_subset]\n",
    "        x_test = [x[0] for x in test_subset]\n",
    "        y_test = [x[1].item() for x in test_subset]\n",
    "        x_train, x_validation, y_train, y_validation = train_test_split(train_validation_x, train_validation_y, test_size=0.1, random_state=42)\n",
    "\n",
    "        train_data_set = CustomEmbeddingDataset(x_train, y_train)\n",
    "        validation_data_set = CustomEmbeddingDataset(x_validation, y_validation)\n",
    "        test_data_set = CustomEmbeddingDataset(x_test, y_test)\n",
    "        \n",
    "        train_loader = DataLoader(train_data_set, batch_size=batch_size, shuffle=True)\n",
    "        validation_loader = DataLoader(validation_data_set, batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_data_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = ConvNet().to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=initial_learning_rate)\n",
    "        \n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=4, min_lr=0.00001)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        best_epoch = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            for images, labels in train_loader:\n",
    "                images = images.unsqueeze(1).to(device) \n",
    "                labels = labels.to(device) \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss = total_loss + loss.item()\n",
    "            \n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for images, labels in validation_loader:\n",
    "                    images = images.unsqueeze(1).to(device) \n",
    "                    labels = labels.to(device) \n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss = val_loss + loss.item()\n",
    "            \n",
    "            val_loss /= len(validation_loader)\n",
    "            file.write(f\"Epoch {epoch+1}, Validation Loss: {val_loss:.4f}, Learning Rate: {optimizer.param_groups[0]['lr']}\\n\")\n",
    "            current_time = datetime.now()\n",
    "            file.write(f\"Start time: {current_time}\\n\")\n",
    "            file.flush()\n",
    "            \n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_epoch = epoch\n",
    "                torch.save(model.state_dict(), f'CNN_best_model_fold_{fold}.pth')\n",
    "            elif epoch - best_epoch >= patience:\n",
    "                file.write(f\"Early stopping at epoch {epoch + 1}\\n\")\n",
    "                file.flush()\n",
    "                break\n",
    "\n",
    "        model.load_state_dict(torch.load(f\"CNN_best_model_fold_{fold}.pth\"))\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        y_true, y_pred = [], []\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        n_class_correct = [0 for _ in range(num_classes)]\n",
    "        n_class_samples = [0 for _ in range(num_classes)]\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images = images.unsqueeze(1) \n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                # max returns (value ,index)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "                y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "                n_samples += labels.size(0)\n",
    "                n_correct += (predicted == labels).sum().item()\n",
    "                for i in range(len(labels)):\n",
    "                    label = labels[i].item()\n",
    "                    pred = predicted[i].item()\n",
    "                    if label == pred:\n",
    "                        n_class_correct[label] += 1\n",
    "                    n_class_samples[label] += 1\n",
    "\n",
    "            acc = 100.0 * n_correct / n_samples\n",
    "            file.write(f\"Accuracy of the network: {acc} %\\n\")\n",
    "            file.flush()\n",
    "\n",
    "            class_accs = []\n",
    "            for i in range(num_classes):\n",
    "                if n_class_samples[i] != 0:\n",
    "                    current_acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "                    class_accs.append(current_acc)\n",
    "                    file.write(f\"Accuracy of {unique_class_names[i]}: {current_acc} %\\n\")\n",
    "                    file.flush()\n",
    "                else:\n",
    "                    class_accs.append(0.0)\n",
    "                    file.write(f\"Accuracy of {unique_class_names[i]}: No samples\\n\")\n",
    "                    file.flush()\n",
    "\n",
    "        f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "        f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "        f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "        jaccard_micro = jaccard_score(y_true, y_pred, average='micro')\n",
    "        jaccard_macro = jaccard_score(y_true, y_pred, average='macro')\n",
    "        jaccard_weighted = jaccard_score(y_true, y_pred, average='weighted')\n",
    "        lb = LabelBinarizer()\n",
    "        y_true_bin = lb.fit_transform(y_true)\n",
    "        y_pred_bin = lb.transform(y_pred)\n",
    "        mccs = [matthews_corrcoef(y_true_bin[:, i], y_pred_bin[:, i]) for i in range(len(lb.classes_))]\n",
    "        average_mcc = sum(mccs) / len(mccs)\n",
    "        hl = hamming_loss(y_true_bin, y_pred_bin)\n",
    "\n",
    "        file.write(f\"Fold: {fold}\\n\")\n",
    "        file.write(f\"Accuracy: {acc:.2f}%\\n\")\n",
    "        file.write(f\"F1 Micro: {f1_micro:.4f}\\n\")\n",
    "        file.write(f\"F1 Macro: {f1_macro:.4f}\\n\")\n",
    "        file.write(f\"F1 Weighted: {f1_weighted:.4f}\\n\")\n",
    "        file.write(f\"Jaccard Micro: {jaccard_micro:.4f}\\n\")\n",
    "        file.write(f\"Jaccard Macro: {jaccard_macro:.4f}\\n\")\n",
    "        file.write(f\"Jaccard Weighted: {jaccard_weighted:.4f}\\n\")\n",
    "        file.write(f\"MCC: {average_mcc:.4f}\\n\")\n",
    "        file.write(f\"Hamming Loss: {hl:.4f}\\n\")\n",
    "        file.flush()\n",
    "\n",
    "        fold_results.append({\n",
    "            'fold': fold,\n",
    "            'accuracy': acc,\n",
    "            'class_accuracy': class_accs,\n",
    "            'f1_micro': f1_micro,\n",
    "            'f1_macro': f1_macro,\n",
    "            'f1_weighted': f1_weighted,\n",
    "            'jaccard_micro': jaccard_micro,\n",
    "            'jaccard_macro': jaccard_macro,\n",
    "            'jaccard_weighted': jaccard_weighted,\n",
    "            'mcc': average_mcc,\n",
    "            'hamming_loss': hl\n",
    "        })\n",
    "\n",
    "    # Compute and print the average metrics\n",
    "    avg_accuracy = sum(result['accuracy'] for result in fold_results) / k_folds\n",
    "    avg_class_accuracy = [sum(result['class_accuracy'][i] for result in fold_results) / k_folds for i in range(num_classes)]\n",
    "    avg_f1_micro = sum(result['f1_micro'] for result in fold_results) / k_folds\n",
    "    avg_f1_macro = sum(result['f1_macro'] for result in fold_results) / k_folds\n",
    "    avg_f1_weighted = sum(result['f1_weighted'] for result in fold_results) / k_folds\n",
    "    avg_jaccard_micro = sum(result['jaccard_micro'] for result in fold_results) / k_folds\n",
    "    avg_jaccard_macro = sum(result['jaccard_macro'] for result in fold_results) / k_folds\n",
    "    avg_jaccard_weighted = sum(result['jaccard_weighted'] for result in fold_results) / k_folds\n",
    "    avg_mcc = sum(result['mcc'] for result in fold_results) / k_folds\n",
    "    avg_hamming_loss = sum(result['hamming_loss'] for result in fold_results) / k_folds\n",
    "\n",
    "    file.write(f\"Average accuracy: {avg_accuracy:.2f}%\\n\")\n",
    "    file.flush()\n",
    "    for i in range(num_classes):\n",
    "        file.write(f\"Average accuracy of {unique_class_names[i]}: {avg_class_accuracy[i]:.2f}%\\n\")\n",
    "    file.write(f\"Average F1 Score (Micro): {avg_f1_micro:.4f}\\n\")\n",
    "    file.write(f\"Average F1 Score (Macro): {avg_f1_macro:.4f}\\n\")\n",
    "    file.write(f\"Average F1 Score (Weighted): {avg_f1_weighted:.4f}\\n\")\n",
    "    file.write(f\"Average Jaccard Score (Micro): {avg_jaccard_micro:.4f}\\n\")\n",
    "    file.write(f\"Average Jaccard Score (Macro): {avg_jaccard_macro:.4f}\\n\")\n",
    "    file.write(f\"Average Jaccard Score (Weighted): {avg_jaccard_weighted:.4f}\\n\")\n",
    "    file.write(f\"Average MCC: {avg_mcc:.4f}\\n\")\n",
    "    file.write(f\"Average Hamming Loss: {avg_hamming_loss:.4f}\\n\")\n",
    "\n",
    "    # Log the end time\n",
    "    end_time = datetime.now()\n",
    "    file.write(f\"End time: {end_time}\\n\")\n",
    "\n",
    "    # Optionally, you can also log the duration\n",
    "    duration = end_time - start_time\n",
    "    file.write(f\"Duration: {duration}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4348213d3ba5b0f93d8cc4a145b996ce6c028cbbcf404838af432985bb8db3e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
